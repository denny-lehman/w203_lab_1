---
title: "Lab 1 Part 1 - Foundational Excercises"
author: "Team03: Savita Chari, Tymon Silva, Denny Lehman "
output: pdf_document
editor_options: 
  chunk_output_type: inline
chunk_output_type: Inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r installing wine data from github, echo=FALSE, include=FALSE}
devtools::install_github("JustinMShea/wooldridge")
```

```{r import packages, echo=FALSE, include=FALSE}
library(dplyr)
library(knitr)
library(tidyverse)
library(wooldridge)
```

# Professional Magic

Your aunt (who is a professional magician), claims to have created a pair of magical coins that share a  connection to each other that makes them land in the same way.  The coins are always flipped at the same time.  For a given flip $i \in \{1,2,3...\}$, let $X_i$ be a Bernoulli random variable representing the outcome of the first coin, and let $Y_i$ be a Bernoulli random variable representing the outcome of the second coin.  You assume that each flip of the pair is independent of all other flips of the pair.  You also assume that

$$P(X_i=0)= P(X_i = 1) = P(Y_i=0) = P(Y_i = 1) = 1/2,$$
and write,

$$P(X_i = Y_i) = p.$$

Your aunt claims that $p > 1/2$.

You design a test to evaluate your aunt's claim.  You flip the coins 3 times and define your test statistic to be the sum $X_1 + Y_1 + X_2 + Y_2 + X_3 + Y_3$

Your null hypothesis is that $p=1/2$.  You plan to reject the null if your test statistic is 0 or 6.


1. What is the type 1 error rate of your test?
2. What is the power of your test for the alternate hypothesis that $p=3/4$?

### 1. What is the type 1 error rate of the test?

The type 1 error occurs when we reject the null hypothesis when it is actually true. Type I error, a false positive, is typically denoted as alpha

$$\alpha = P(reject \space Ho|Ho)$$
We are given that Ho is when p = 1/2. p is the probability that Xi = Yi

$$ H_o: p=\frac{1}{2}, \space p=P(X_i = Y_i)$$
We know that each flip of the pair has the following potential outcomes:
$$
P(X_i=Y_i) = P(X=0,Y=0) + P(X=1,Y=1)\\
P(X_i=Y_i|H_o) = 0.5 = P(X=0,Y=0) + P(X=1,Y=1)\\
P(X=0,Y=0)=\frac{1}{4}\\
P(X=1,Y=1)=\frac{1}{4}
$$
We are also given that our test statistic, theta_hat, will be equal to 

$$
\theta =X_1 +Y_1+X_2+Y_2+X_3+Y_3
$$

From the problem definition, we reject the null when our test statistic theta is 0 or 6 after 3 flips of pairs of coins. Type I error is thus
$$
\alpha = P(reject \space Ho|Ho)\\
\alpha = P(\theta \in \{0,6\} |p=\frac{1}{2})\\
\alpha = P(\theta=0|p=\frac{1}{2})+P(\theta=6|p=\frac{1}{2})
$$
Solve for each term using the binomial distribution formula

$$
binomial \space distribution \space formula \\
P_x = nCx\space p^x(1-p)^{n-x}\\ \\
solve \space for \space P(\theta=0|p=\frac{1}{2})\\
P(X_i=0) + P(Y_i=0) = \frac{1}{4} \\
P(\theta=0|p=\frac{1}{2}) = 0C3 \space p^0*(1-p)^3\\
P(\theta=0|p=\frac{1}{2}) = (1) \space (.5)^0*(.5)^3
$$

result of a pair of flips both coming up tails is
$$
0 = X_i + Y_i \\
P(trial = 0) = P(X_i=0) * P(Y_i=0) \\
P(one \space trial = 0) = \frac{1}{2} * \frac{1}{2} = \frac{1}{4}
\\
P(\theta=0) = \frac{1}{4}^3
$$
$$
2 = X_i + Y_i \\
P(trial = 2) = P(X_i=1) * P(Y_i=1) \\
P(one \space trial = 2) = \frac{1}{2} * \frac{1}{2} = \frac{1}{4}
\\
\theta = 3 \space trials \\
P(\theta=6) = \frac{1}{4}^3
$$
now sub back in for alpha
$$
\alpha = P(\theta=0|H_o) + P(\theta=6|H_o) \\
\alpha = \frac{1}{4}^3 + \frac{1}{4}^3 \\
\alpha = \frac{2}{64} = \frac{1}{32}
$$

### 2. What is the power of your test for the alternate hypothesis that $p=3/4$?
c
$$Power = 1 - \beta $$
Where $$\beta$$ is the type 2 error, or the false negative rate. Type 2 error occurs when we fail to reject the null hypothesis when we should.

$$
Power = P(reject \space null|H_a) \\
P(X_i=Y_i) = p = \frac{3}{4}
\\
\beta = P(fail \space to \space  reject \space H_o|H_a)
$$
## Wrong Test, Right Data
### $Our$ $Analysis:$

The response categories in Likert scales creates rank order/ordinal data, but the intervals between their values cannot be compared or presumed equal. Therefore, standard statistical analysis such as  the mean, standard deviation etc. are inappropriate for ordinal data. Hence, this data is not suitable for a paired t-test. Paired t-test is parametric which means it expects a normal distribution of metric scaled data in its test population. It may be used for ordinal data such as True/False but for our use case a non-parametric test such as  Wilcoxon signed-rank test or Sign test would be a better  choice.   


# Test Assumptions
For the four following questions, your task is to evaluate the assumptions for the given test using your background knowledge and examining the data.

### World Happiness
You would like to know whether people in countries with high GDP per capita (higher than the mean) are more happy or less happy than people in countries with low GDP (lower than the mean). List all assumptions for a two-sample t-test and evaluate them.
```{r read in data, echo=FALSE, include=FALSE}
WHRdata <- read_csv("datasets/happiness_WHR.csv")
```

```{r inspect data, echo=FALSE, include=FALSE}
WHRdata
```

```{r Data wrangling and test setup, echo=FALSE, include=FALSE}
WHRmean <- mean(WHRdata$`Log GDP per capita`, na.rm = TRUE)

WHR <- WHRdata %>%
  mutate(
    'High_Low_GDP' = case_when(
      `Log GDP per capita` < WHRmean ~ 'Low', 
       `Log GDP per capita` > WHRmean ~ 'High',
    )
  )

WHRsubset <- WHR %>%
  select(High_Low_GDP
         , `Life Ladder`
         ) %>%
  filter(High_Low_GDP %in% c('High', 'Low'))

count_high <- count(filter(WHRsubset, High_Low_GDP == "High"))
count_low <- count(filter(WHRsubset, High_Low_GDP == "Low"))
count_high
count_low

```
##### Two-sample t-test assumptions
1. Metric variables  
The Life Ladder variable is a metric variable. We can justify this by reviewing the FAQ for this dataset that states, "... it asks respondents to think of a ladder, with the best possible life for them being a 10, and the worst possible life being a 0. They are then asked to rate their own current lives on that 0 to 10 scale." From this description, we know the responses fall on a 0 to 10 scale, where the distances or gaps between each point on the scale are the same, since each rung on a ladder is the same distance for the next.

2. IID  
Here, we assume IID is upheld since Gallup\footnote{https://www.gallup.com/corporate/212381/who-we-are.aspx} is a reputable research firm, and we expect their sampling and data collection processes maintain the independence and identically distributed assumption.

3. Data is normally distributed (check sample size)  
Since we have sufficient sample sizes (greater than 30) in each of the Low (n=105) and High (n=121) groups for GDP, we can assume normality by the Central Limit Theorem.

Thus, all the assumptions for a two-sample t-test are valid and the test would be appropriate. 

## Study question 3: Legislator age
You would like to test whether Democratic or Republican senators are older. List all assumptions for a Wilcoxon rank-sum test and evaluate them.

## Hypothesis
#### H_0: Probability that the Democrat legislators are older than Republicans is same as the probability of Republican legislators being older than their Democrat counterpart.

## What is Wilcoxon rank-sum test?
Wilcoxon rank-sum test is a non-parametric, distribution-free test for two independent samples. Though it involves assumptions, but those assumptions are less restrictive than the assumptions for parametric tests. It considers the ranks instead of the metric value of the variable. It uses the order of variables to construct statistics that can be used to test hypothesis.

$Advantage:$ The population distribution doesn't have to be normal, so it's easier to justify a rank-based test. It is a good choice of test for smaller sample size.

$Disadvantage:$ Since these tests do not use the metric information they loose statistical power.

### The Wilcox rank-sum test works on following principals:
  1. The samples are interval scale and list the score from lowest to highest. Higher rank gets higher score
  2. Only considers rank instead of looking at metric value of the variable
  3. Uses order of variables to construct statistics that is used for hypothesis
  
### The Wilcox rank-sum test makes the following assumptions:
  1. Test involves independent samples
  2. The test is non-parametric i.e. the population is distribution-free
  3. Works on Un-Paired Data

```{r DATA Read, echo=FALSE, include=FALSE}
setwd("C:\\Users\\savit\\lab_1\\datasets")
legislator_full_data <- read_csv("legislators-current.csv")
```
##  Data and Methodology

The Data set has 538 rows and 34 columns. But we need a Subset of this data  to answer the research question
```{r Get the shape of the data in terms of number of rows and column, echo=FALSE, include=FALSE}
Num_Of_Rows <- nrow(legislator_full_data)
Num_Of_Columns <- ncol(legislator_full_data)
sprintf('The Dataset had %i Rows and %i Columns', Num_Of_Rows ,Num_Of_Columns)
```
### In order to perform Wilcoxon test we need 2 types of columns from our dataset
1. $[X]$ A categorical column with 2 distinct groups : We chose the 'party' column for this requirement but the data set includes more than two categories. We filtered only the Republican and Democrat data in order to perform Wilcoxon rank-sum test as we are interested in only those categories. Thus we fulfill the requirement of having only 2 categories.

2. $[Y]$ A statistical column with numeric outcome: We chose the 'birthday' field for this requirement. Though this field is of class 'Date', with some manipulation we can make it a numeric outcome

```{r Data Cleaning, echo=FALSE, include=FALSE}
legislator_subset_data <- legislator_full_data %>%
  select (full_name, birthday, party)  %>%
    filter(party != 'Independent')

```
### Data Transformation
As mentioned above, the birthday field is of 'Date' class and is in YYYY/MM/DD format. In order to find out the age of the legislator we calculate the current age of the legislator by doing a datediff between current date and the birthday

```{r Find the Age of every legislator, echo=FALSE, include=TRUE}
currentDate <- Sys.Date()
legislator_DemRep_Age_data  <- legislator_subset_data %>% 
  mutate(AgeInYears = round((as.numeric(difftime(currentDate , birthday,units = "weeks")))/52))
legislator_DemRep_Age_data[1:3,]
```
### Exploring Assumption 1 : Are the data sets independent?
Well, yes, a legislator cannot belong to 2 parties simultaneously, so the data is independent. The source of the data is the congress-legislators project. Data such as information (first, last, etc.), biographical information (birthday, gender) of every members of the United States Congress (1789-Present) is maintained. Data of one legislator does not depend on the other.

### Exploring Assumption 2 : Is the population distribution-free?
From the graph below we can tell that the age distribution between republicans and democrats is some what normal, and not too skewed  Moreover, as per Central Limit Theorem (CMT), when we have a sufficiently large sample size (500+ in our case), the sampling distribution starts to approximate a normal distribution .

### $Given$ $this$ $information,$ $Wilcoxon$ $rank-sum$ $tes$t $is$ $not$ $a$ $good$ $choic$e $for$ $this$ $use$ $case$

```{r Age Distribution , echo=FALSE, include=TRUE}
par(mfrow=c(1,3))
plot(legislator_DemRep_Age_data$AgeInYears,col ='green4',  main = "Age distribution of all Legistrators",  ylab='Age in years', xlab='Number of legistrators')

hist((legislator_DemRep_Age_data %>% select(party,AgeInYears)  %>%
    filter(party == 'Democrat'))$AgeInYears, main = "Age distribution of Democrats", col='blue3', xlab='Age in years')

hist((legislator_DemRep_Age_data %>% select(party,AgeInYears)  %>%
    filter(party == 'Republican'))$AgeInYears, main = "Age distribution of Republicans", col='red3', xlab='Age in years')


```

Also, from the box plot below, it is very clear that the we can calculate variance on the metric data.  Democrats have a bigger variance than the Republicans wrt. their age. In the Republican data set there is an outlier too.

```{r Calculate the variance in Age for Democrats and Republicans, echo=FALSE, include=TRUE}
boxplot(legislator_DemRep_Age_data$AgeInYears~legislator_DemRep_Age_data$party, main=  "Variance in the Age", xlab = "Political Party", ylab = "Age in Years")
```

### Performing two sided Wilcoxon rank-sum test
In order to prove the null hypothesis we perform the Wilcoxon rank-sum test. our data satisfies the requirements for conducting this test.

```{r Wilcoxon rank-sum test}

wilcox.test(legislator_DemRep_Age_data$AgeInYears~legislator_DemRep_Age_data$party, mu=0, 
            alt = "two.sided", conf.int = T, conf.level=0.95,  paired=F, exact=F, correct=T)
```

#### With p-value being less that 0.05, We are rejecting the null hypothesis. 
Which stated that the probability that the Democrat legislators are older than Republicans is same as the probability of Republican legislators being older than their Democrat counterpart.

## Observation
### It is our opinion that in this scenario where we have an option to run either parametric or non-parametric test, we should choose parametric t-test becasue it has more statistical power as compared to non-parametric test such as wilcoxon Rank-Sum test.

Our data fulfills all the requirements to run a t-test

1. The samples are independent of one another and are Metric scale.
2. The populations have equal variance or spread
3. The populations are normally distributed (iid)
```{r}
t.test(legislator_DemRep_Age_data$AgeInYears~legislator_DemRep_Age_data$party, mu=0, 
       alt = "two.sided", conf=0.95, var.eq=F, paired=F)
```
#### We are rejecting the null hypothesis. 
Which stated that the probability that the Democrat legislators are older than Republicans is same as the probability of Republican legislators being older than their Democrat counterpart.
The t-test further observes that the mean age of Democrat legislators is 2.60071 years higher than their Republican counterpart.

## Test Outcome and it's correctness
Our test sample size is >30 so t-test can be performed

1. We used mean to create our test statistics which is well accepted  distributions for statistical testing.
2. we followed the decision rules of a hypothesis testing which gives us the guarantee that our false positive rejection rate (the type 1 error rate) is bounded.  
3. Our test is statistically significant as our p-value is less than 0.05 

 
## Conclusion for Study question 3: Legislator age

It is important to observe the data before choosing the right kind of test to make correct predictions and to leverage the full power of statistics.


# It's for your health!
You would like to use it [wine dataset] to test whether countries have more deaths from heart disease or from liver disease. List all assumptions for a signed-rank test and evaluate them.

```{r load wine dataset from wooldrigde, echo=FALSE}
data("wine")
wine
```

##### Wilcoxon signed-rank test assumptions
1. Metric variables
We need metric variables for a paired Wilcoxon signed-rank test, and the heart and liver variables satisfy this assumption. The Heart and Liver variables represent the number of deaths from heart and liver disease in each country. 

2. IID  
There are some concerns with IID for this data. Since there dataset only contains 21 countries, we do not know how these countries were drawn, particularly if they were randomly select. Furthermore, we do not know how the data was collected, which means the data draw could not have been identically distributed.

3. Difference is symmetric
The last assumption requires the difference between pairs follows a symmetric distribution. If you look at the histogram below of the difference between heart and liver disease deaths, one could argue that the distribution is skewed left, which violates our difference is symmetric assumption.

Due to major concerns with assumptions 2 and 3, the Wilcoxon signed-rank test would not be a valid test to perform to answer the question. 

```{r transfrom wine data and plot hist, echo=FALSE, include=TRUE, fig.width=4, fig.height=3}
wine_transform <- wine %>%
  mutate(heart_liver = heart - liver)

hist(wine_transform$heart_liver, breaks = 15)
```


# Positive vibes
You would like to test whether the US population feels more positive towards Protestants or towards Catholics. List all assumption for a paired t-test and evaluate them.

```{r read data, echo=FALSE, message=FALSE, error=FALSE}
GSSdata <- read_csv("datasets/GSS_religion.csv")

head(GSSdata)
```

```{r Data setup, echo=FALSE, include=FALSE}
# WHRmean <- mean(WHRdata$`Log GDP per capita`, na.rm = TRUE)
# 
# WHR <- WHRdata %>%
#   mutate(
#     'High_Low_GDP' = case_when(
#       `Log GDP per capita` < WHRmean ~ 'Low', 
#        `Log GDP per capita` > WHRmean ~ 'High',
#     )
#   )
# 
# WHRsubset <- WHR %>%
#   select(High_Low_GDP
#          , `Life Ladder`
#          ) %>%
#   filter(High_Low_GDP %in% c('High', 'Low'))
# 
# count_high <- count(filter(WHRsubset, High_Low_GDP == "High"))
# count_low <- count(filter(WHRsubset, High_Low_GDP == "Low"))
# count_high
# count_low

```

##### Paired two-sample t-test assumptions
1. Metric variables  
The Life Ladder variable is a metric variable. We can justify this by reviewing the FAQ for this dataset that states, "... it asks respondents to think of a ladder, with the best possible life for them being a 10, and the worst possible life being a 0. They are then asked to rate their own current lives on that 0 to 10 scale." From this description, we know the responses fall on a 0 to 10 scale, where the distances or gaps between each point on the scale are the same, since each rung on a ladder is the same distance for the next.

2. IID  
Here, we assume IID is upheld since Gallup\footnote{https://www.gallup.com/corporate/212381/who-we-are.aspx} is a reputable research firm, and we expect their sampling and data collection processes maintain the independence and identically distributed assumption.

3. Data is normally distributed (check sample size)  
Since we have sufficient sample sizes (greater than 30) in each of the Low (n=105) and High (n=121) groups for GDP, we can assume normality by the Central Limit Theorem.

Thus, all the assumptions for a two-sample t-test are valid and the test would be appropriate. 
